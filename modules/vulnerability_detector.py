#!/usr/bin/env python3
"""
Vulnerability Detection Inference Script

A professional-grade refactored version of the original vulnerability detection code.
Supports multiple model architectures for code vulnerability classification.

Usage:
    python vulnerability_detector.py --config config.yaml
    python vulnerability_detector.py --data_file test.csv --model_path checkpoint.bin
"""

from __future__ import annotations

from modules.mask import preprocess_and_mask

import json
import logging
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    precision_recall_fscore_support,
)
from torch.utils.data import DataLoader, Dataset, SequentialSampler
from transformers import (
    AutoConfig,
    AutoModel,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    BertConfig,
    BertForMaskedLM,
    BertTokenizer,
    RobertaConfig,
    RobertaForSequenceClassification,
    RobertaTokenizer,
    T5Config,
    T5ForConditionalGeneration,
)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ModelConfig:
    """Configuration for model inference."""
    
    # Data configuration
    data_file: str = ""
    data_type: str = "csv"  # csv, json, adv
    output_file: Optional[str] = None
    
    # Model configuration
    model_type: str = "roberta"
    model_name_or_path: str = "microsoft/codebert-base"
    tokenizer_name: Optional[str] = None
    config_name: Optional[str] = None
    checkpoint_path: Optional[str] = None
    
    # Training configuration
    num_labels: int = 4
    block_size: int = -1
    batch_size: int = 32
    do_lower_case: bool = False
    
    # Hardware configuration
    device: Optional[str] = None
    no_cuda: bool = False
    
    def __post_init__(self):
        """Post-initialization validation and defaults."""
        if not self.tokenizer_name:
            self.tokenizer_name = self.model_name_or_path
        if not self.config_name:
            self.config_name = self.model_name_or_path
        if self.device is None:
            self.device = "cuda" if torch.cuda.is_available() and not self.no_cuda else "cpu"


@dataclass
class InputFeatures:
    """Container for input features."""
    input_tokens: Optional[List[str]]
    input_ids: List[int]
    idx: str
    label: int


@dataclass
class InferenceResult:
    """Container for inference results."""
    predictions: np.ndarray
    labels: np.ndarray
    logits: Optional[np.ndarray] = None
    metrics: Optional[Dict[str, float]] = None


class ModelRegistry:
    """Registry for supported model architectures."""
    
    SUPPORTED_MODELS = {
        'codet5': (T5Config, T5ForConditionalGeneration, RobertaTokenizer),
        'bert': (BertConfig, BertForMaskedLM, BertTokenizer),
        'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),
        'codegen': (AutoConfig, AutoModel, AutoTokenizer),
        'codellama': (AutoConfig, AutoModel, AutoTokenizer),
    }
    
    @classmethod
    def get_model_classes(cls, model_type: str) -> Tuple[type, type, type]:
        """Get model configuration, model class, and tokenizer class."""
        if model_type not in cls.SUPPORTED_MODELS:
            raise ValueError(f"Unsupported model type: {model_type}")
        return cls.SUPPORTED_MODELS[model_type]


class CodePreprocessor:
    """Handles code preprocessing and masking operations."""
    
    @staticmethod
    def preprocess_code(code: str, language: str = 'cpp') -> str:
        """
        Preprocess code by normalizing whitespace.
        
        Note: This is a simplified version. The original uses a mask module
        that should implement proper code preprocessing and masking.
        """
        # Mask Veriable
        masked, _ = preprocess_and_mask(
        code,
        language='cpp',
        remove_whitespace='normalize'
        )
        # Normalize whitespace
        code = masked.replace('\n', ' ').replace('\t', ' ')
        # Remove extra whitespace
        code = ' '.join(code.split())
        return code


class VulnerabilityDataset(Dataset):
    """Dataset class for vulnerability detection."""
    
    def __init__(self, tokenizer, config: ModelConfig, file_path: str):
        """Initialize dataset."""
        self.tokenizer = tokenizer
        self.config = config
        self.examples: List[InputFeatures] = []
        self.preprocessor = CodePreprocessor()
        
        self._load_data(file_path)
    
    def _load_data(self, file_path: str) -> None:
        """Load data from file based on data type."""
        logger.info(f"Loading data from {file_path}")
        
        if self.config.data_type == 'json':
            self._load_json_data(file_path)
        elif self.config.data_type in ['csv', 'adv']:
            self._load_csv_data(file_path)
        else:
            raise ValueError(f"Unsupported data type: {self.config.data_type}")
        
        logger.info(f"Loaded {len(self.examples)} examples")
    
    def _load_json_data(self, file_path: str) -> None:
        """Load data from JSON file."""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for i, item in enumerate(data):
            features = self._convert_to_features(item, i)
            self.examples.append(features)
    
    def _load_csv_data(self, file_path: str) -> None:
        """Load data from CSV file."""
        data = pd.read_csv(file_path)
        
        for idx, row in data.iterrows():
            features = self._convert_to_features(row.to_dict(), idx)
            self.examples.append(features)
    
    def _convert_to_features(self, data: Dict[str, Any], idx: int) -> InputFeatures:
        """Convert raw data to input features."""
        # Extract fields based on data type
        if self.config.data_type == 'json':
            code = data['code']
            label = data.get('label', -1)
        elif self.config.data_type == 'csv':
            code = data['processed_func']
            label = data.get('CWE ID', -1)
        elif self.config.data_type == 'adv':
            code = data['perturbated_code']
            label = 1
        else:
            raise ValueError(f"Unsupported data type: {self.config.data_type}")
        
        # Preprocess code
        processed_code = self.preprocessor.preprocess_code(code)
        
        # Tokenize based on model type
        if self.config.model_type in ["codet5", "t5", "codegen", "codellama"]:
            input_ids = self._tokenize_generative_model(processed_code)
            input_tokens = None
        else:
            input_tokens, input_ids = self._tokenize_encoder_model(processed_code)
        
        return InputFeatures(
            input_tokens=input_tokens,
            input_ids=input_ids,
            idx=str(idx),
            label=label
        )
    
    def _tokenize_generative_model(self, code: str) -> List[int]:
        """Tokenize for generative models (T5, CodeGen, etc.)."""
        return self.tokenizer.encode(
            code.split("</s>")[0],
            max_length=self.config.block_size,
            padding='max_length',
            truncation=True,
        )
    
    def _tokenize_encoder_model(self, code: str) -> Tuple[List[str], List[int]]:
        """Tokenize for encoder models (BERT, RoBERTa, etc.)."""
        tokens = self.tokenizer.tokenize(code)
        tokens = tokens[:self.config.block_size - 2]
        
        # Add special tokens
        source_tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]
        source_ids = self.tokenizer.convert_tokens_to_ids(source_tokens)
        
        # Add padding
        padding_length = self.config.block_size - len(source_ids)
        source_ids += [self.tokenizer.pad_token_id] * padding_length
        
        return source_tokens, source_ids
    
    def __len__(self) -> int:
        """Return dataset size."""
        return len(self.examples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get item by index."""
        features = self.examples[idx]
        return (
            torch.tensor(features.input_ids, dtype=torch.long),
            torch.tensor(features.label, dtype=torch.long)
        )


class VulnerabilityModel(nn.Module):
    """Enhanced vulnerability detection model."""
    
    def __init__(self, encoder, config: ModelConfig, tokenizer):
        """Initialize the model."""
        super().__init__()
        self.encoder = encoder
        self.config = config
        self.tokenizer = tokenizer
        self.num_labels = config.num_labels
        
        # Determine if encoder has built-in classifier
        self.has_builtin_classifier = (
            hasattr(encoder, 'classifier') or 
            hasattr(encoder, 'config') and 
            getattr(encoder.config, 'num_labels', None) == self.num_labels
        )
        
        # Add classification head if needed
        if not self.has_builtin_classifier:
            hidden_size = self._get_hidden_size()
            if hidden_size:
                self.classifier = nn.Linear(hidden_size, self.num_labels)
            else:
                self.classifier = None
    
    def _get_hidden_size(self) -> Optional[int]:
        """Get hidden size from encoder config."""
        if hasattr(self.encoder, 'config'):
            config = self.encoder.config
            return getattr(config, 'hidden_size', None)
        return getattr(self.encoder, 'hidden_size', None)
    
    def _get_pad_token_id(self) -> int:
        """Get padding token ID."""
        return getattr(self.tokenizer, 'pad_token_id', 1)
    
    def _pool_hidden_states(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """Pool hidden states based on model type."""
        if hidden_states.dim() == 2:
            return hidden_states
        
        # Use CLS token for BERT-like models
        model_type = getattr(self.encoder.config, 'model_type', '').lower()
        if model_type in {'bert', 'roberta', 'deberta', 'xlm-roberta', 'electra'}:
            return hidden_states[:, 0, :]
        
        # Mean pooling for other models
        mask = attention_mask.unsqueeze(-1).float()
        summed = (hidden_states * mask).sum(dim=1)
        lengths = mask.sum(dim=1).clamp(min=1e-9)
        return summed / lengths
    
    def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """Forward pass."""
        pad_token_id = self._get_pad_token_id()
        attention_mask = input_ids.ne(pad_token_id)
        
        # Get encoder outputs
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # Get logits
        logits = getattr(outputs, 'logits', None)
        
        if logits is None:
            # Get hidden states and pool them
            if hasattr(outputs, 'last_hidden_state'):
                hidden_states = outputs.last_hidden_state
            else:
                hidden_states = outputs[0] if isinstance(outputs, (tuple, list)) else outputs
            
            pooled = self._pool_hidden_states(hidden_states, attention_mask)
            
            # Initialize classifier if needed
            if self.classifier is None:
                hidden_size = pooled.size(-1)
                self.classifier = nn.Linear(hidden_size, self.num_labels).to(pooled.device)
            
            logits = self.classifier(pooled)
        
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)
            return loss, logits
        
        return logits


class MetricsCalculator:
    """Calculate various evaluation metrics."""
    
    @staticmethod
    def calculate_metrics(labels: np.ndarray, predictions: np.ndarray) -> Dict[str, float]:
        """Calculate comprehensive metrics."""
        # Basic metrics
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='macro', zero_division=0
        )
        
        # Confusion matrix based metrics
        cm = confusion_matrix(labels, predictions)
        metrics = MetricsCalculator._calculate_cm_metrics(cm)
        
        return {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1),
            **metrics
        }
    
    @staticmethod
    def _calculate_cm_metrics(cm: np.ndarray) -> Dict[str, float]:
        """Calculate metrics from confusion matrix."""
        num_classes = cm.shape[0]
        total = cm.sum()
        
        tnr_list, fpr_list, fnr_list = [], [], []
        
        for c in range(num_classes):
            tp = cm[c, c]
            fp = cm[:, c].sum() - tp
            fn = cm[c, :].sum() - tp
            tn = total - (tp + fp + fn)
            
            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
            fnr = fn / (tp + fn) if (tp + fn) > 0 else 0.0
            
            tnr_list.append(tnr)
            fpr_list.append(fpr)
            fnr_list.append(fnr)
        
        return {
            'tnr': float(np.mean(tnr_list)),
            'fpr': float(np.mean(fpr_list)),
            'fnr': float(np.mean(fnr_list)),
        }


class ModelLoader:
    """Handle model and tokenizer loading."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.registry = ModelRegistry()
    
    def load_tokenizer_and_base_model(self) -> Tuple[Any, Any, Any]:
        """Load configuration, tokenizer, and base model."""
        config_class, model_class, tokenizer_class = self.registry.get_model_classes(
            self.config.model_type
        )
        
        # Load configuration
        model_config = config_class.from_pretrained(self.config.config_name)
        model_config.num_labels = self.config.num_labels
        
        # Load tokenizer
        tokenizer = self._load_tokenizer(tokenizer_class)
        
        # Determine block size
        self._set_block_size(tokenizer)
        
        # Load base model
        base_model = model_class.from_pretrained(
            self.config.model_name_or_path,
            config=model_config
        )
        
        return model_config, tokenizer, base_model
    
    def _load_tokenizer(self, tokenizer_class) -> Any:
        """Load and configure tokenizer."""
        kwargs = {'do_lower_case': self.config.do_lower_case}
        
        if self.config.model_type in ['codegen', 'codellama']:
            kwargs['trust_remote_code'] = True
        
        tokenizer = tokenizer_class.from_pretrained(
            self.config.tokenizer_name,
            **kwargs
        )
        
        # Set padding token if needed
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            if tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id
        
        return tokenizer
    
    def _set_block_size(self, tokenizer) -> None:
        """Set block size based on tokenizer capabilities."""
        if self.config.block_size <= 0:
            max_len = getattr(tokenizer, 'model_max_length', 512)
            if max_len == int(1e30):  # Handle large default values
                max_len = 512
            self.config.block_size = max_len
        else:
            max_len = getattr(tokenizer, 'model_max_length', 512)
            self.config.block_size = min(self.config.block_size, max_len)


class VulnerabilityDetector:
    """Main vulnerability detection class."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.device = torch.device(config.device)
        self.model_loader = ModelLoader(config)
        self.metrics_calculator = MetricsCalculator()
        
    def load_model(self) -> Tuple[Any, VulnerabilityModel]:
        """Load model and tokenizer."""
        logger.info("Loading model and tokenizer...")
        
        model_config, tokenizer, base_model = self.model_loader.load_tokenizer_and_base_model()
        
        # Build classifier model
        model = VulnerabilityModel(base_model, self.config, tokenizer)
        
        # Load checkpoint if provided
        if self.config.checkpoint_path:
            self._load_checkpoint(model)
        
        model.to(self.device)
        model.eval()
        
        logger.info(f"Model loaded on device: {self.device}")
        return tokenizer, model
    
    def _load_checkpoint(self, model: VulnerabilityModel) -> None:
        """Load model checkpoint."""
        logger.info(f"Loading checkpoint from {self.config.checkpoint_path}")
        
        checkpoint_path = Path(self.config.checkpoint_path)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
        
        if checkpoint_path.suffix == '.safetensors':
            try:
                from safetensors.torch import load_file
                state_dict = load_file(checkpoint_path)
            except ImportError:
                raise ImportError("safetensors not installed. Install with: pip install safetensors")
        else:
            state_dict = torch.load(checkpoint_path, map_location='cpu')
        
        model.load_state_dict(state_dict, strict=False)
        logger.info("Checkpoint loaded successfully")
    
    def predict(self, data_file: str) -> InferenceResult:
        """Run inference on data file."""
        logger.info(f"Running inference on {data_file}")
        
        # Validate input file exists
        if not Path(data_file).exists():
            raise FileNotFoundError(f"Data file not found: {data_file}")
        
        try:
            tokenizer, model = self.load_model()
            
            # Create dataset and dataloader
            dataset = VulnerabilityDataset(tokenizer, self.config, data_file)
            sampler = SequentialSampler(dataset)
            dataloader = DataLoader(
                dataset, 
                sampler=sampler, 
                batch_size=self.config.batch_size
            )
            
            # Run inference
            all_logits, all_labels = self._run_inference_loop(model, dataloader)
            
            # Process results
            logits = np.concatenate(all_logits, axis=0)
            labels = np.concatenate(all_labels, axis=0)
            predictions = np.argmax(logits, axis=1)
            
            logger.info(f"Processed {len(predictions)} samples")
            
            # Calculate metrics if labels are available
            metrics = None
            if (labels >= 0).any():
                try:
                    metrics = self.metrics_calculator.calculate_metrics(labels, predictions)
                    logger.info(f"Metrics calculated: {metrics}")
                except Exception as e:
                    logger.warning(f"Failed to calculate metrics: {e}")
            
            result = InferenceResult(
                predictions=predictions,
                labels=labels,
                logits=logits,
                metrics=metrics
            )
            
            # Save results if output file specified
            if self.config.output_file:
                try:
                    self._save_results(result)
                except Exception as e:
                    logger.error(f"Failed to save results: {e}")
                    # Continue without saving
            
            return result
            
        except Exception as e:
            logger.error(f"Error during inference: {e}")
            raise
    
    def _run_inference_loop(self, model: VulnerabilityModel, dataloader: DataLoader) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """Run the actual inference loop."""
        all_logits, all_labels = [], []
        
        logger.info("Running inference...")
        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i % 100 == 0:
                    logger.info(f"Processing batch {i+1}/{len(dataloader)}")
                
                inputs, labels = batch
                inputs = inputs.to(self.device)
                labels = labels.to(self.device)
                
                logits = model(inputs)
                
                all_logits.append(logits.detach().cpu().numpy())
                all_labels.append(labels.detach().cpu().numpy())
        
        logger.info("Inference completed")
        return all_logits, all_labels
    
    def _save_results(self, result: InferenceResult) -> None:
        """Save inference results."""
        output_path = Path(self.config.output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if output_path.suffix.lower() == '.npy':
            np.save(output_path, result.predictions)
        else:
            # Save as CSV with additional information
            df = pd.DataFrame({
                'prediction': result.predictions,
                'label': result.labels,
            })
            
            # Save metrics as separate comment file and regular CSV
            if result.metrics:
                # Save metrics to separate file
                metrics_path = output_path.with_suffix('.metrics.txt')
                with open(metrics_path, 'w') as f:
                    f.write("Evaluation Metrics:\n")
                    f.write("==================\n")
                    for key, value in result.metrics.items():
                        f.write(f"{key.capitalize()}: {value:.4f}\n")
                logger.info(f"Metrics saved to {metrics_path}")
            
            # Save regular CSV
            df.to_csv(output_path, index=False, float_format='%.4f')
        
        logger.info(f"Results saved to {output_path}")


def main():
    """Main function for CLI usage."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Vulnerability Detection Inference")
    
    # Data arguments
    parser.add_argument("--data_file", type=str, required=True,
                       help="Path to input data file")
    parser.add_argument("--data_type", type=str, default="csv",
                       choices=["csv", "json", "adv"],
                       help="Type of input data")
    parser.add_argument("--output_file", type=str,
                       help="Path to save predictions")
    
    # Model arguments
    parser.add_argument("--model_type", type=str, default="roberta",
                       choices=list(ModelRegistry.SUPPORTED_MODELS.keys()),
                       help="Type of model to use")
    parser.add_argument("--model_name_or_path", type=str,
                       default="microsoft/codebert-base",
                       help="Model name or path")
    parser.add_argument("--checkpoint_path", type=str,
                       help="Path to model checkpoint")
    parser.add_argument("--num_labels", type=int, default=4,
                       help="Number of classification labels")
    
    # Training arguments
    parser.add_argument("--block_size", type=int, default=-1,
                       help="Maximum sequence length")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="Inference batch size")
    
    # Hardware arguments
    parser.add_argument("--no_cuda", action="store_true",
                       help="Disable CUDA")
    
    args = parser.parse_args()
    
    # Convert to config
    config = ModelConfig(
        data_file=args.data_file,
        data_type=args.data_type,
        output_file=args.output_file,
        model_type=args.model_type,
        model_name_or_path=args.model_name_or_path,
        checkpoint_path=args.checkpoint_path,
        num_labels=args.num_labels,
        block_size=args.block_size,
        batch_size=args.batch_size,
        no_cuda=args.no_cuda,
    )
    
    # Run inference
    detector = VulnerabilityDetector(config)
    result = detector.predict(args.data_file)
    
    print(f"Inference completed. Predictions shape: {result.predictions.shape}")
    if result.metrics:
        print("Metrics:")
        for key, value in result.metrics.items():
            print(f"  {key}: {value:.4f}")


if __name__ == "__main__":
    main()